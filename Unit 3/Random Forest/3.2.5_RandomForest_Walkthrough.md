

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```


```python
file = 'C:\\Users\\Abe\\Data Science Bootcamp\\Unit 3\\Random Forest\\LoanStats3d.csv'
col = list(range(2,111))
tp = pd.read_csv(file, skipinitialspace=True, header=1, usecols=col, iterator=True, chunksize=1000, dtype={19:str, 55:str})
df = pd.concat(tp, ignore_index=True)
print (df.shape)
df.head()
```

    (421097, 109)
    




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loan_amnt</th>
      <th>funded_amnt</th>
      <th>funded_amnt_inv</th>
      <th>term</th>
      <th>int_rate</th>
      <th>installment</th>
      <th>grade</th>
      <th>sub_grade</th>
      <th>emp_title</th>
      <th>emp_length</th>
      <th>...</th>
      <th>num_tl_90g_dpd_24m</th>
      <th>num_tl_op_past_12m</th>
      <th>pct_tl_nvr_dlq</th>
      <th>percent_bc_gt_75</th>
      <th>pub_rec_bankruptcies</th>
      <th>tax_liens</th>
      <th>tot_hi_cred_lim</th>
      <th>total_bal_ex_mort</th>
      <th>total_bc_limit</th>
      <th>total_il_high_credit_limit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18000.0</td>
      <td>18000.0</td>
      <td>18000.0</td>
      <td>36 months</td>
      <td>7.49%</td>
      <td>559.83</td>
      <td>A</td>
      <td>A4</td>
      <td>Systems Support Engineer</td>
      <td>5 years</td>
      <td>...</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>97.8</td>
      <td>50.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>335119.0</td>
      <td>102660.0</td>
      <td>20400.0</td>
      <td>107522.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7200.0</td>
      <td>7200.0</td>
      <td>7200.0</td>
      <td>36 months</td>
      <td>17.27%</td>
      <td>257.67</td>
      <td>D</td>
      <td>D3</td>
      <td>Department Manager</td>
      <td>9 years</td>
      <td>...</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>94.1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>22400.0</td>
      <td>7302.0</td>
      <td>16900.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>36 months</td>
      <td>9.80%</td>
      <td>160.87</td>
      <td>B</td>
      <td>B3</td>
      <td>Accounts Payable Manager</td>
      <td>6 years</td>
      <td>...</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>100.0</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>155131.0</td>
      <td>20751.0</td>
      <td>0.0</td>
      <td>23281.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15000.0</td>
      <td>15000.0</td>
      <td>15000.0</td>
      <td>60 months</td>
      <td>13.44%</td>
      <td>344.69</td>
      <td>C</td>
      <td>C3</td>
      <td>Counselor</td>
      <td>4 years</td>
      <td>...</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>100.0</td>
      <td>40.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>176208.0</td>
      <td>155521.0</td>
      <td>13800.0</td>
      <td>95668.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3600.0</td>
      <td>3600.0</td>
      <td>3600.0</td>
      <td>36 months</td>
      <td>11.48%</td>
      <td>118.68</td>
      <td>B</td>
      <td>B5</td>
      <td>Systems Administrator</td>
      <td>&lt; 1 year</td>
      <td>...</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>100.0</td>
      <td>28.6</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>457315.0</td>
      <td>42332.0</td>
      <td>22000.0</td>
      <td>43306.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 109 columns</p>
</div>




```python
# Use random sample of one tenth of the data so I don't get a memory error
df_ss = df.sample(frac=.1)
```


```python
# categorical = df_ss.select_dtypes(include=['object'])
# for i in categorical:
#     column = categorical[i]
#     print(i)
#     print(column.nunique())
```


```python
# Convert ID and Interest Rate to numeric.
# df_ss['id'] = pd.to_numeric(df_ss['id'], errors='coerce')
df_ss['int_rate'] = pd.to_numeric(
                    df_ss['int_rate'].str.strip('%'), errors='coerce')

# Drop other columns with many unique variables
df_ss.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line',
            'revol_util', 'sub_grade', 'addr_state', 'desc'],
            1, inplace=True)
```


```python
from sklearn import ensemble
from sklearn.model_selection import cross_val_score

rfc = ensemble.RandomForestClassifier()
X = df_ss.ix[:, df_ss.columns != 'loan_status']
Y = df_ss['loan_status']
X = pd.get_dummies(X)
X = X.dropna(axis=1)

forest = cross_val_score(rfc, X, Y, cv=10)
print (forest)
print (forest.mean())
```

    c:\users\abe\appdata\local\programs\python\python36-32\lib\site-packages\sklearn\model_selection\_split.py:581: Warning: The least populated class in y has only 7 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.
      % (min_groups, self.n_splits)), Warning)
    

    [ 0.97840532  0.97887491  0.97982435  0.9786274   0.97838993  0.98147708
      0.98171456  0.97980518  0.98170587  0.97837452]
    0.979719912876
    

# End of Guidance
## Goals:
Reduce to minimum # of features while keeping accuracy of a 10 fold random forest > 90. First by PCA, second by using only the highest correlated features.


```python
Y.groupby(Y).count()
```




    loan_status
    Charged Off            3960
    Current               25789
    Default                   7
    Fully Paid            11003
    In Grace Period         451
    Late (16-30 days)       143
    Late (31-120 days)      757
    Name: loan_status, dtype: int64




```python
# Drop the loan status of 'Default'
df_ss.drop(df_ss[df_ss['loan_status'] == 'Default'].index,inplace=True)

df_ss['loan_status'].groupby(df_ss['loan_status']).count()
```




    loan_status
    Charged Off            3960
    Current               25789
    Fully Paid            11003
    In Grace Period         451
    Late (16-30 days)       143
    Late (31-120 days)      757
    Name: loan_status, dtype: int64




```python
# Redo random forest without the error

rfc = ensemble.RandomForestClassifier()
X = df_ss.ix[:, df_ss.columns != 'loan_status']
Y = df_ss['loan_status']
X = pd.get_dummies(X)
X = X.dropna(axis=1)

scre = cross_val_score(rfc, X, Y, cv=10)
print(scre)
print(scre.mean())
```

    [ 0.97863755  0.98005698  0.98005698  0.98052257  0.98052257  0.97980998
      0.97909739  0.97980518  0.98028035  0.97908745]
    0.979787698809
    


```python
# Rank feature importances

rfc.fit(X,Y)
importances = rfc.feature_importances_
imp_df = pd.DataFrame()
imp_df['features'] = X.columns
imp_df['importance'] = importances

imp_df.sort(columns='importance',ascending=False,inplace=True)
imp_df.head(10)
```

    c:\users\abe\appdata\local\programs\python\python36-32\lib\site-packages\ipykernel\__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
    




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>features</th>
      <th>importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13</th>
      <td>out_prncp</td>
      <td>0.263789</td>
    </tr>
    <tr>
      <th>127</th>
      <td>last_pymnt_d_Apr-2017</td>
      <td>0.102479</td>
    </tr>
    <tr>
      <th>14</th>
      <td>out_prncp_inv</td>
      <td>0.097086</td>
    </tr>
    <tr>
      <th>17</th>
      <td>total_rec_prncp</td>
      <td>0.072514</td>
    </tr>
    <tr>
      <th>158</th>
      <td>last_credit_pull_d_Apr-2017</td>
      <td>0.054032</td>
    </tr>
    <tr>
      <th>15</th>
      <td>total_pymnt</td>
      <td>0.044960</td>
    </tr>
    <tr>
      <th>16</th>
      <td>total_pymnt_inv</td>
      <td>0.038719</td>
    </tr>
    <tr>
      <th>155</th>
      <td>next_pymnt_d_May-2017</td>
      <td>0.036582</td>
    </tr>
    <tr>
      <th>22</th>
      <td>last_pymnt_amnt</td>
      <td>0.029665</td>
    </tr>
    <tr>
      <th>18</th>
      <td>total_rec_int</td>
      <td>0.028268</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Use PCA to reduce the number of features
from sklearn.decomposition import PCA 

sklearn_pca = PCA(n_components=12)
X_sklearn = sklearn_pca.fit_transform(X,Y)

print(sklearn_pca.explained_variance_ratio_)

scre = cross_val_score(rfc, X_sklearn, Y, cv=10)
print('\n')
print(scre)
print('\n')
print(scre.mean())
```

    [  8.47332162e-01   6.60639563e-02   4.42055111e-02   2.39565451e-02
       7.09169354e-03   4.11173586e-03   2.37878619e-03   1.92770086e-03
       9.67562210e-04   8.64666340e-04   6.26873251e-04   2.46208584e-04]
    
    
    [ 0.96059815  0.95987654  0.96130104  0.96128266  0.95795724  0.96009501
      0.95914489  0.96103588  0.9605607   0.9598384 ]
    
    
    0.960169052821
    


```python
# Turn Y variables into dummies to run a correlation matrix

# Get dummies for Y and then combine with X and its dummies
y_dum = pd.get_dummies(Y)
y_df = pd.concat([y_dum, X], axis=1)

# Create correlation matrix and combine the absolute values of the y dummy correlation for an idea of total correlation
corrmat = y_df.corr()
y_ = np.sum(np.absolute(corrmat.ix[:, corrmat.columns[0:5]]),axis=1)
y_ = pd.DataFrame(y_, columns=['correlation'])
y_cols = y_dum.columns.tolist()
y_ = y_[~y_.index.isin(y_cols)]
y_.sort_values('correlation',ascending=False,inplace=True)
y_
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>correlation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>next_pymnt_d_May-2017</th>
      <td>2.287731</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Apr-2017</th>
      <td>2.006477</td>
    </tr>
    <tr>
      <th>last_credit_pull_d_Apr-2017</th>
      <td>1.587972</td>
    </tr>
    <tr>
      <th>out_prncp_inv</th>
      <td>1.503087</td>
    </tr>
    <tr>
      <th>out_prncp</th>
      <td>1.503057</td>
    </tr>
    <tr>
      <th>last_pymnt_amnt</th>
      <td>1.489638</td>
    </tr>
    <tr>
      <th>total_rec_prncp</th>
      <td>1.204031</td>
    </tr>
    <tr>
      <th>total_pymnt_inv</th>
      <td>0.947057</td>
    </tr>
    <tr>
      <th>total_pymnt</th>
      <td>0.947037</td>
    </tr>
    <tr>
      <th>collection_recovery_fee</th>
      <td>0.779298</td>
    </tr>
    <tr>
      <th>recoveries</th>
      <td>0.775978</td>
    </tr>
    <tr>
      <th>last_credit_pull_d_Oct-2016</th>
      <td>0.658134</td>
    </tr>
    <tr>
      <th>last_credit_pull_d_Feb-2017</th>
      <td>0.644736</td>
    </tr>
    <tr>
      <th>total_rec_int</th>
      <td>0.578911</td>
    </tr>
    <tr>
      <th>last_credit_pull_d_Mar-2017</th>
      <td>0.512189</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Mar-2017</th>
      <td>0.510033</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Jul-2016</th>
      <td>0.477277</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Nov-2016</th>
      <td>0.468352</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Aug-2016</th>
      <td>0.466878</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Sep-2016</th>
      <td>0.465551</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Mar-2016</th>
      <td>0.457268</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Oct-2016</th>
      <td>0.452238</td>
    </tr>
    <tr>
      <th>int_rate</th>
      <td>0.451089</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Jun-2016</th>
      <td>0.445289</td>
    </tr>
    <tr>
      <th>last_pymnt_d_May-2016</th>
      <td>0.438429</td>
    </tr>
    <tr>
      <th>last_credit_pull_d_Jan-2017</th>
      <td>0.435230</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Apr-2016</th>
      <td>0.411170</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Jan-2017</th>
      <td>0.399771</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Feb-2017</th>
      <td>0.394517</td>
    </tr>
    <tr>
      <th>last_pymnt_d_Feb-2016</th>
      <td>0.391039</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>title_Vacation</th>
      <td>0.022176</td>
    </tr>
    <tr>
      <th>emp_length_9 years</th>
      <td>0.022096</td>
    </tr>
    <tr>
      <th>emp_length_3 years</th>
      <td>0.021364</td>
    </tr>
    <tr>
      <th>purpose_vacation</th>
      <td>0.021301</td>
    </tr>
    <tr>
      <th>acc_now_delinq</th>
      <td>0.020726</td>
    </tr>
    <tr>
      <th>emp_length_1 year</th>
      <td>0.020576</td>
    </tr>
    <tr>
      <th>application_type_JOINT</th>
      <td>0.019656</td>
    </tr>
    <tr>
      <th>verification_status_joint_Not Verified</th>
      <td>0.019656</td>
    </tr>
    <tr>
      <th>application_type_INDIVIDUAL</th>
      <td>0.019656</td>
    </tr>
    <tr>
      <th>issue_d_Jul-2015</th>
      <td>0.019652</td>
    </tr>
    <tr>
      <th>emp_length_5 years</th>
      <td>0.016682</td>
    </tr>
    <tr>
      <th>emp_length_&lt; 1 year</th>
      <td>0.016474</td>
    </tr>
    <tr>
      <th>home_ownership_OWN</th>
      <td>0.015553</td>
    </tr>
    <tr>
      <th>chargeoff_within_12_mths</th>
      <td>0.014790</td>
    </tr>
    <tr>
      <th>emp_length_4 years</th>
      <td>0.014221</td>
    </tr>
    <tr>
      <th>title_Green loan</th>
      <td>0.012190</td>
    </tr>
    <tr>
      <th>num_tl_30dpd</th>
      <td>0.011697</td>
    </tr>
    <tr>
      <th>emp_length_6 years</th>
      <td>0.011425</td>
    </tr>
    <tr>
      <th>pymnt_plan_n</th>
      <td>0.011388</td>
    </tr>
    <tr>
      <th>pymnt_plan_y</th>
      <td>0.011388</td>
    </tr>
    <tr>
      <th>purpose_renewable_energy</th>
      <td>0.010885</td>
    </tr>
    <tr>
      <th>verification_status_Source Verified</th>
      <td>0.010829</td>
    </tr>
    <tr>
      <th>title_Major purchase</th>
      <td>0.010804</td>
    </tr>
    <tr>
      <th>emp_length_2 years</th>
      <td>0.010744</td>
    </tr>
    <tr>
      <th>delinq_amnt</th>
      <td>0.010577</td>
    </tr>
    <tr>
      <th>purpose_major_purchase</th>
      <td>0.010036</td>
    </tr>
    <tr>
      <th>title_new day</th>
      <td>0.009137</td>
    </tr>
    <tr>
      <th>next_pymnt_d_Jun-2017</th>
      <td>0.009137</td>
    </tr>
    <tr>
      <th>last_credit_pull_d_Dec-2014</th>
      <td>0.009137</td>
    </tr>
    <tr>
      <th>policy_code</th>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>188 rows Ã— 1 columns</p>
</div>




```python
# Select how many of the top features we want to use
x_ = y_.index[0:6].tolist()

X = df_ss.ix[:, df_ss.columns != 'loan_status']
X = pd.get_dummies(X)
Y = df_ss['loan_status']
X = X.ix[:, X.columns.isin(x_)]

forest = cross_val_score(rfc, X, Y, cv=10)
print (forest)
print (forest.mean())
x_
```

    [ 0.9404225   0.93708452  0.93494777  0.93705463  0.93562945  0.94299287
      0.93230404  0.93799002  0.93751485  0.93726236]
    0.937320301603
    




    ['next_pymnt_d_May-2017',
     'last_pymnt_d_Apr-2017',
     'last_credit_pull_d_Apr-2017',
     'out_prncp_inv',
     'out_prncp',
     'last_pymnt_amnt']


