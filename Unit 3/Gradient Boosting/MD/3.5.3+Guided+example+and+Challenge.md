

```python
import pandas as pd
import numpy as np
import scipy
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn import ensemble
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
```

### Gradient boost guided example

Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner.


```python
df = pd.read_csv((
    "https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/"
    "master/ESS_practice_data/ESSdata_Thinkful.csv")).dropna()

# Definine outcome and predictors.
# Set our outcome to 0 and 1.
y = df['partner'] - 1
X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]

# Make the categorical variable 'country' into dummies.
X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)

# Create training and test sets.
offset = int(X.shape[0] * 0.9)

# Put 90% of the data in the training set.
X_train, y_train = X[:offset], y[:offset]

# And put 10% in the test set.
X_test, y_test = X[offset:], y[offset:]
```

Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are "deviance", or "exponential".  Deviance is used for logistic regression, and we'll try that here.


```python
# We'll make 500 iterations, use 2-deep trees, and set our loss function.
params = {'n_estimators': 1000,
          'max_depth': 4,
          'loss': 'deviance',
         }

# Initialize and fit the model.
clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train, y_train)

predict_train = clf.predict(X_train)
predict_test = clf.predict(X_test)

# Accuracy tables.
table_train = pd.crosstab(y_train, predict_train, margins=True)
table_test = pd.crosstab(y_test, predict_test, margins=True)

train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']
train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']

test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']
test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']

print((
    'Training set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}\n\n'
    'Test set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}'
).format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))
```

    Training set accuracy:
    Percent Type I errors: 0.011320240043644299
    Percent Type II errors: 0.08074195308237861
    
    Test set accuracy:
    Percent Type I errors: 0.10429447852760736
    Percent Type II errors: 0.18036809815950922
    

Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface.  But they aren't quite a black box.  We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees.


```python
feature_importance = clf.feature_importances_

# Make importances relative to max importance.
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
plt.subplot(1, 2, 2)
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, X.columns[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.show()
```


![png](output_6_0.png)


It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner.

### DRILL: Improve this gradient boost model

While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:

* Creating new features
* Applying more overfitting-prevention strategies like subsampling
* More iterations
* Trying a different loss function
* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications

Have fun!


```python
df.describe()
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>idno</th>
      <th>year</th>
      <th>tvtot</th>
      <th>ppltrst</th>
      <th>pplfair</th>
      <th>pplhlp</th>
      <th>happy</th>
      <th>sclmeet</th>
      <th>sclact</th>
      <th>gndr</th>
      <th>agea</th>
      <th>partner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>8.147000e+03</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
      <td>8147.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>4.022622e+04</td>
      <td>6.500307</td>
      <td>3.818584</td>
      <td>5.574936</td>
      <td>6.005155</td>
      <td>5.321468</td>
      <td>7.708482</td>
      <td>5.215908</td>
      <td>2.755984</td>
      <td>1.496379</td>
      <td>47.262182</td>
      <td>1.384681</td>
    </tr>
    <tr>
      <th>std</th>
      <td>6.320721e+05</td>
      <td>0.500031</td>
      <td>2.008937</td>
      <td>2.215745</td>
      <td>2.120127</td>
      <td>2.166217</td>
      <td>1.720839</td>
      <td>1.438792</td>
      <td>0.901406</td>
      <td>0.500018</td>
      <td>18.316890</td>
      <td>0.486550</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000e+00</td>
      <td>6.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>15.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.063000e+03</td>
      <td>6.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>7.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>33.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.749000e+03</td>
      <td>7.000000</td>
      <td>4.000000</td>
      <td>6.000000</td>
      <td>6.000000</td>
      <td>5.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>47.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.778000e+03</td>
      <td>7.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>8.000000</td>
      <td>7.000000</td>
      <td>9.000000</td>
      <td>6.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>61.000000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.100143e+07</td>
      <td>7.000000</td>
      <td>7.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>7.000000</td>
      <td>5.000000</td>
      <td>2.000000</td>
      <td>114.000000</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
corrmat = df.corr()
plt.figure(figsize=(12,10))
sns.heatmap(corrmat)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x5d43f50>




![png](output_10_1.png)



```python
# New Features

# People which have a positive outlook of other people
df['positive_people'] = np.where((df['ppltrst'] > 5) & (df['pplfair'] > 5) & (df['pplhlp'] > 5), 1,0)

# People over 30
df['relationship_age'] = np.where(df['agea'] > 30, 1, 0)
```


```python
# Set up dataset columns
y = df['partner'] - 1

X = df.loc[:, ~df.columns.isin(df[['partner','cntry','idno','year']])]
X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)
```


```python
X_train = X.sample(frac=.9)
X_test = X.drop(X_train.index)
y_train = y.ix[X_train.index]
y_test = y.drop(X_train.index)
```


```python
# Set up boosted classifier

params = {'max_depth': 4,
          'n_estimators': 500,
          'loss': 'deviance'
         }

clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train,y_train)

predict_train = clf.predict(X_train)
predict_test = clf.predict(X_test)

# Set Up Tables

training_accuracy = pd.DataFrame(confusion_matrix(y_train, predict_train))
training_accuracy['All'] = training_accuracy.sum(axis=1)
training_accuracy.loc['All'] = training_accuracy.sum(axis=0)

testing_accuracy = pd.DataFrame(confusion_matrix(y_test, predict_test))
testing_accuracy['All'] = testing_accuracy.sum(axis=1)
testing_accuracy.loc['All'] = testing_accuracy.sum(axis=0)

train_tI = training_accuracy.ix[0,1] / training_accuracy.ix['All','All']
train_tII = training_accuracy.ix[1,0] / training_accuracy.ix['All','All']

test_tI = testing_accuracy.ix[0,1] / testing_accuracy.ix['All','All']
test_tII = testing_accuracy.ix[1,0] / testing_accuracy.ix['All','All']

print((
    'Training set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}\n\n'
    'Test set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}'
).format(train_tI, train_tII, test_tI, test_tII))
```

    Training set accuracy:
    Percent Type I errors: 0.020049099836333878
    Percent Type II errors: 0.11988543371522095
    
    Test set accuracy:
    Percent Type I errors: 0.053987730061349694
    Percent Type II errors: 0.19631901840490798
    


```python
# Adding Depth

params = {'max_depth': 8,
          'n_estimators': 500,
          'loss': 'deviance'
         }

clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train,y_train)

predict_train = clf.predict(X_train)
predict_test = clf.predict(X_test)

# Set Up Tables

training_accuracy = pd.DataFrame(confusion_matrix(y_train, predict_train))
training_accuracy['All'] = training_accuracy.sum(axis=1)
training_accuracy.loc['All'] = training_accuracy.sum(axis=0)

testing_accuracy = pd.DataFrame(confusion_matrix(y_test, predict_test))
testing_accuracy['All'] = testing_accuracy.sum(axis=1)
testing_accuracy.loc['All'] = testing_accuracy.sum(axis=0)

train_tI = training_accuracy.ix[0,1] / training_accuracy.ix['All','All']
train_tII = training_accuracy.ix[1,0] / training_accuracy.ix['All','All']

test_tI = testing_accuracy.ix[0,1] / testing_accuracy.ix['All','All']
test_tII = testing_accuracy.ix[1,0] / testing_accuracy.ix['All','All']

print((
    'Training set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}\n\n'
    'Test set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}'
).format(train_tI, train_tII, test_tI, test_tII))
```

    Training set accuracy:
    Percent Type I errors: 0.0
    Percent Type II errors: 0.0
    
    Test set accuracy:
    Percent Type I errors: 0.08711656441717791
    Percent Type II errors: 0.17791411042944785
    


```python
# Adding more estimators

params = {'max_depth': 4,
          'n_estimators': 1000,
          'loss': 'deviance'
         }

clf = ensemble.GradientBoostingClassifier(**params)
clf.fit(X_train,y_train)

predict_train = clf.predict(X_train)
predict_test = clf.predict(X_test)

# Set Up Tables

training_accuracy = pd.DataFrame(confusion_matrix(y_train, predict_train))
training_accuracy['All'] = training_accuracy.sum(axis=1)
training_accuracy.loc['All'] = training_accuracy.sum(axis=0)

testing_accuracy = pd.DataFrame(confusion_matrix(y_test, predict_test))
testing_accuracy['All'] = testing_accuracy.sum(axis=1)
testing_accuracy.loc['All'] = testing_accuracy.sum(axis=0)

train_tI = training_accuracy.ix[0,1] / training_accuracy.ix['All','All']
train_tII = training_accuracy.ix[1,0] / training_accuracy.ix['All','All']

test_tI = testing_accuracy.ix[0,1] / testing_accuracy.ix['All','All']
test_tII = testing_accuracy.ix[1,0] / testing_accuracy.ix['All','All']

print((
    'Training set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}\n\n'
    'Test set accuracy:\n'
    'Percent Type I errors: {}\n'
    'Percent Type II errors: {}'
).format(train_tI, train_tII, test_tI, test_tII))
```

    Training set accuracy:
    Percent Type I errors: 0.01309328968903437
    Percent Type II errors: 0.07910529187124932
    
    Test set accuracy:
    Percent Type I errors: 0.0638036809815951
    Percent Type II errors: 0.18773006134969325
    
